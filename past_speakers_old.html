<!DOCTYPE HTML>
<!--
	Spectral by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Past Speakers</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Page Wrapper -->
			<div id="page-wrapper">

				<!-- Header -->
					<header id="header">
						<h1><a href="index.html">Deep Learning @ UGA</a></h1>
						<nav id="nav">
							<ul>
								<li class="special">
									<a href="#menu" class="menuToggle"><span>Menu</span></a>
									<div id="menu">
										<ul>
											<li><a href="index.html">Home</a></li>
											<li><a href="schedule.html">Schedule</a></li>
											<li><a href="resources.html">Resources</a></li>
											<li><a href="recordings.html">Recordings</a></li>
											<li><a href="past_speakers.html">Past Speakers</a></li>
											<li><a href="about_us.html">About Us</a></li>
											<li><a href="alumni.html">Alumni</a></li>

										</ul>
									</div>
								</li>
							</ul>
						</nav>
					</header>					

					<section class="spotlight">
						<div class="image"><img src="speakers/chethan_pandarinath_120318/chethan_pandarinath_120318.jpg" alt="" />
						</div><div class="content">
							<h2><a href="chethan_pandarinath_120318.html">Inferring single-trial neural population dynamics using sequential auto-encoders</a></h2>
							<p>
								Neuroscience is experiencing a revolution in which simultaneous recording of thousands of neurons is revealing population dynamics that are not apparent from single-neuron responses. This structure is typically extracted from data averaged across many trials, but deeper understanding requires studying phenomena detected in single trials, which is challenging due to incomplete sampling of the neural population, trial-to-trial variability, and fluctuations in action potential timing. We introduce latent factor analysis via dynamical systems, a deep learning method to infer latent dynamics from single-trial neural spiking data. When applied to a variety of macaque and human motor cortical datasets, latent factor analysis via dynamical systems accurately predicts observed behavioral variables, extracts precise firing rate estimates of neural dynamics on single trials, infers perturbations to those dynamics that correlate with behavioral choices, and combines data from non-overlapping record- ing sessions spanning months to improve inference of underlying dynamics.
							</p>

					
							<p style="font-size: 12px;"><strong>Dr. Pandarinath</strong>
								is an Assistant Professor of Biomedical Engineering and Neurosurgery at Emory University and Georgia Tech. His research centers on understanding how large populations of neurons in the brain perform computations and represent intention, and using these insights to develop high-performance, robust, and practical assistive devices for people with disabilities and neurological disorders.

							<br>
							<br>
								Dr. Pandarinath did his undergraduate training at North Carolina State University in Computer Engineering and Physics, and his PhD in Electrical Engineering at Cornell University. He worked with Prof. Sheila Nirenberg to use machine learning and information theory to understand how the eye transmits information to the brain, and to develop strategies to restore vision in cases of retinal degeneration, based on optogenetic stimulation of retinal ganglion cells. Then, as postdoc at Stanford, he worked with Profs. Jaimie Henderson (Neurosurgery) and Krishna Shenoy (Electrical Engineering) to develop intracortical brain-machine interfaces (BMIs) for people with paralysis, as part of the BrainGate2 clinical trial. His work demonstrated the fastest communication rates ever shown by people with paralysis using a BMI. Most recently, he has focused on developing methods to uncover the dynamics of large ensembles of neurons, using techniques from an emerging field of computer science and artificial intelligence known as deep learning.
							<br>
							<br>
								Dr. Pandarinath was recently named a K12 scholar through the Interdisciplinary Rehabilitation Engineering Research Career Development Program administered by Northwestern University, and was previously a Stanford Dean’s Fellow, a finalist for the Sammy Kuo Award in Neuroscience at Stanford, and received a postdoctoral fellowship from the Crain H. Neilsen Foundation for Spinal Cord Injury Research.
							</p>
						
						</div>
					</section>

					<section class="spotlight">
						<div class="image"><img src="speakers/david_nicholson_112618/david_nicholson_112618.jpg" alt="" /></div><div class="content">
							<h2><a href="david_nicholson_112618.html">A comparative study of neural network architectures for segmentation: finding syllables in birdsong</a></h2>
								<p>
									Vocalizations such as speech can be segmented into units like words or phonemes. Neural networks for speech to text typically avoid segmenting into units of time, instead mapping directly from the input features to a sequence of text. Neglecting the segmentation problem allows these networks to achieve high accuracy. However there are many cases where it is desirable to find segments, such as diagnosis of speech disorders.
								<br>
								<br>
									Another case where it is desirable to find segments is in birdsong. Songbirds learn their songs as juveniles from adult tutors, much like a baby learns to talk from its parents. Many neuroscientists study songbirds to understand how brains learn and produce speech and similar motor skills like playing the piano. However the field has been held back by the inability to fully automate the process of segmenting song into its elements, often called syllables, and then label those syllables. Because of this bottleneck, often only a small portion of the thousands of songs collected in a behavioral experiment can be analyzed.
								<br>
								<br>
									Previously, my collaborators and I have shown that birdsong provides a good testbed for networks that segment vocalizations, and that a hybrid convolutional-recurrent neural network can outperform previously proposed architectures that segment birdsong into syllables.
								</p>

									<p><a href="https://youtu.be/1XEDFpUGmqs">Video</a></p>

									<p><a href = "https://nickledave.github.io/neural-network-segment-birdsong.html">Github Page</a></p>
								<p>
									However it remains unclear whether networks for segmentation require recurrent connections, or whether alternatively fully convolutional architectures can recover segments. It is also unclear how robust the different architectures are to noise. Initial results suggest that two types of convolutional networks (encoder-decoder and dilated convolutional) yield segmentation as good as a network with a recurrent connection. Furthermore, the convolutational networks can be trained in about a third of the time. Experiments in progress test how robust these networks are to the presence of noise. I will discuss whether recurrent connections are always advantageous in neural networks or whether convolutional architectures can always be competitive, provided the input can be represented as an image.
								</p>
							</p>

							<p style="font-size: 12px;"><strong>Dr. Nicholson</strong>
								is a neuroscientist at Emory University in Atlanta, Georgia.
							</p>
						</div>
					</section>

					<section class="spotlight">
						<div class="image"><img src="speakers/fred_hohman_110518/fred_hohman_110518.jpg" alt="" /></div><div class="content">
							<h2><a href="fred_hohman_110518.html">Visual Analytics in Deep Learning: Highlights from An Interrogative Survey</a></h2>
							<p>
								Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W's and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.
							</p>

							<p style="font-size: 12px;"><strong>Fred Hohman</strong>
								is a PhD student at Georgia Tech's College of Computing. His research combines principles from human-computer interaction and techniques from machine learning to improve deep learning interpretability using interactive data visualization. He won the 2018 NASA Space Technology Research Fellowship and Microsoft AI for Earth Award for using AI to improve sustainability. He received his B.S. in mathematics and physics from the University of Georgia, and has conducted research at Microsoft Research, NASA Jet Propulsion Laboratory, and Pacific Northwest National Laboratory.
							</p>
						</div>
					</section>


					<section class="spotlight">
						<div class="image"><img src="speakers/sheng_li_102218/sheng_li_102218.jpg" alt="" /></div><div class="content">
							<h2><a href="sheng_li_102218.html">Adversarial Training for Sequential Data</a></h2>
							<p>
								Existing approaches on sequential modeling mostly serve as ‘black-box’ methods and are limited in their ability to interpret the results. Besides, these sophisticated learning models are known to be susceptible to deliberate adversarial attacks. While there exist several defense strategies against adversarial attacks on static data, these approaches do not take into account the structural property of sequential data. To close this gap, we develop a new learning framework based on adversarial training to improve the robustness to sequence classification. Specifically, it answers the question of When and How to perturb the data when applying the adversarial training. I will demonstrate the effectiveness of the proposed method in a diversity of real-world datasets, such as e-Commerce, natural language processing and remote sensing. In addition, I will briefly introduce my recent deep learning projects on action recognition and answer selection.
							</p>

							<p style="font-size: 12px;"><strong>Dr. Li</strong>
								is an Assistant Professor at the Department of Computer Science, University of Georgia. He received his Ph.D. degree in computer engineering from Northeastern University, Boston, MA in 2017, and worked as a research scientist at Adobe Research, San Jose, CA from 2017 to 2018. He has published over 65 papers at leading conferences and journals, and has received three best paper awards or nominations at SDM 2014, IEEE ICME 2014, and IEEE FG 2013. He serves as Associate Editor of IEEE Computational Intelligence Magazine, Neurcomputing, IET Image Processing, and Journal of Electronic Imaging. He also serves as SPC member for AAAI, and PC member for NIPS, IJCAI, KDD, ICLR, etc. His research interests include robust machine learning, representation learning, visual intelligence and causal inference.
							</p>
						</div>
					</section>



					<section class="spotlight">
						<div class="image"><img src="speakers/stephane_pinel_092418/stephane_pinel_092418_resized.jpg" alt="" /></div><div class="content">
							<h2><a href="stephane_pinel_092418.html">From A/B testing to Markov and Bayes</a></h2>
							<p>
								In this talk we will review the continuum from
								A/B testing to contextual multi-armed bandits, to markov decision processes, and ultimately deep reinforcement learning. One of the challenges faced by reinforcement learning is the need for large amount of data for proper training. We will explore how probabilistic
								programming (Bayesian inference) can help with generating powerful environment models that can in turn enable training when only little, eventually non-stationary,  and noisy data are available.
							</p>

							<p style="font-size: 12px;"><strong>Dr.	Stephane Pinel</strong>
									is a Sr. Manager of Data Science and Solution Architect. He has worked on the development of large scale recommendation engines, search engines and delivery via API at Cox Automotive, Merchandise and Financial Planning (MFP) solutions, supply
									chain management management solutions for large retailer (e.g. ToysRus, Walgreens, Best Buy...). He is currently managing 2 Data Science teams at MailChimp in the areas of look-alike-audience expansion and customer engagement metrics such as customer life value.
									His current research includes Artificial Intelligence and Automated decision making via a combination of Deep Reinforcement Learning and Probabilistic Programming (deep bayesian modeling, variational inference) as well as serverless microservice architecture
									for Data Science, “Big Data” data flow cloud architecture (AWS EMR/Spark, AWS lambda, AWS Kinesis, AWS API gateway, Data Pipeline…)). From 2006 to 2010, he was the CTO and Co-founder of Sayana Wireless, a startup company developing 60GHz multi-gigabit low power
									low cost radio for Consumer Electronics WPAN applications such as Wireless HDMI.  In 2000, he received a Ph.D. in microelectronics and microsystems. He has published over 170 journals/proceeding papers,8 patents, 29 invention disclosure, over 3200 Google Scholar
									citations, and contributed to 2 wireless standards (and was vice-chair of ECMA committee) and book chapters. He gave numerous invited talks and organized numerous workshops at international conferences, managed over 30 PhD student, postdoc and research engineers
									and numerous industrial collaboration projects. His research interests included Advanced semiconductor devices modelling, millimeter-waves radio circuits and electromagnetic designs, Neural Network and Genetic Algorithms based designs, advanced 3D RF and millimeter-waves
									integration and antenna packaging technologies, RF-MEMS and micro-machining techniques.
							</p>
						</div>
					</section>



					<section class="spotlight">
						<div class="image"><img src="speakers/raunak_dey_091018/raunak_dey_091018_resized.jpg" alt="" /></div><div class="content">
							<h2><a href="raunak_dey_091018.html">Complementary Segmentation – En Route towards “Quasi-Perfect” Segmentation</a></h2>
							<p>
								The go to method for image segmentation 
								(Extracting an object of interest from a picture) has been U-Nets 
								and its variants since its inception at MICCAI 2015 by Dr Olaf Ronneberger 
								(Google Deep Mind and Albert-Ludwigs-Universität Freiburg). In this talk we 
								are going to discuss the basic idea of segmentation, followed by segmentation 
								with focus not only on the region of interest, but also on the complementary 
								section which is for the most part unused. We will see how the inclusion of 
								this “seemingly meaningless” complementary section improves the quality of 
								segmentation over traditional U-nets, as well as enable the networks to 
								perform under conditions where the U-nets and its likes are expected to 
								fail. The paper of Comp-Net has been accepted for Spotlight presentation 
								at this year’s rendition of MICCAI at Granada, Spain, and we will discuss 
								the limitations and motivations which led to the inception of Comp-Net with 
								the application to Skull Stripping for Brain MRI. We will end the discussion 
								with few neat tips and tricks which I believe are a must know for anyone 
								researching into Deep Learning and which has been instrumental in my own research.
							</p>

							<p style="font-size: 12px;"><strong>Raunak Dey</strong> 
								is a PhD student in the Department of Computer Science 
								at the University of Georgia, working under the guidance of Dr Yi Hong. 
								His area of interest is primarily Artificial Intelligence and Heuristics, 
								and his current work is in the application of AI to the field of biomedical imaging.
							</p>
						</div>
					</section>



					<section class="spotlight">
						<div class="image"><img src="speakers/andrew_king_041318/andrew_king_041318_resized.jpg" alt="" /></div><div class="content">
							<h2><a href="andrew_king_041318.html">Semantic Deep Learning of Underwater Ecology</a></h2>
							<p>
								A fundamental issue limiting ecological studies in marine environments,
								such as coral reefs, is the difficulty of generating accurate and
								repeatable maps of the underlying ecosystems. We examine two major
								deep learning methods to assist with this task, patch-based convolutional
								neural network approaches and fully convolutional network models.
								We explore 2D and 3D semantic map creation using these methods on
								image data extracted from underwater video. For our patch-based
								CNN approaches we use individual point-wise ground truth annotations.
								For our fully convolutional networks we develop a tool for the fast
								creation of ground truth image segmentations.
							</p>

							<p style="font-size: 12px;"><strong>Andrew King</strong>
								is a machine learning scientist currently completing a master’s
								degree in Artificial Intelligence at the University of Georgia.
								His research as a master’s student has focused on deep learning
								models for semantic segmentation. He is the author of Deep Segments,
								Scopi, and a number of other machine learning and computer
								vision based applications.
							</p>
						</div>
					</section>





					<section class="spotlight">
						<div class="image"><img src="speakers/layton_hayes_030218/layton_hayes_030218_resized.jpg" alt="" /></div><div class="content">
							<h2><a href="layton_hayes_030218.html">Memory-Augmented Neural Networks</a></h2>
							<p>
								Memory Augmented Neural Networks (MANNs) are a relatively new
								class of architectures that improve the memory performance of
								neural networks significantly beyond that of traditional
								Recurrent Neural Networks. They have exciting applications for
								learned planning, meta learning, and one-shot learning.
							</p>

							<p style="font-size: 12px;"><strong>Layton Hayes</strong> is a Master's
							student in the AI institute. He spent last
							summer interning at Amazon and climbing mountains in the PNW,
							and this summer he'll be at Facebook and climbing the Sierra
							Nevadas. His research interests focus on Deep Reinforcement
							Learning, MANNs, and Human-Machine Interface. His real interests
							focus on space-future, dank memes, and building superhero gadgets.
							</p>
						</div>
					</section>


					<section class="spotlight">
						<div class="image"><img src="speakers/rajeswari_sivakumar_020218/rajeswari_sivakumar_020218_resized.jpg" alt="" /></div><div class="content">
							<h2><a href="rajeswari_sivakumar_020218.html">Generative Adversarial Networks</a></h2>
							<p>
								Generative Adversarial Networks (GANs) are an exciting new
								deep learning technique pioneered by Ian Goodfellow and his
								research lab. This new model describes a framework for training
								competing models that perform opposite tasks: generating data
								and discriminating the veracity of training instances. By
								training a generative and discriminative model we can tackle
								a range of tasks from generating more accurate representations
								of real data as well as improving semi-supervised learning techniques.
							</p>

							<p style="font-size: 12px;"><strong>Rajeswari Sivakumar</strong> 
								is a 2nd year graduate student at the University of Georgia,
								studying Artificial Intelligence. Her research interests include
								applications of machine learning in biomedical image analysis.
								She is currently examining deep learning approaches to classifying
								brain images.
							</p>
						</div>
					</section>


					<section class="spotlight">
						<div class="image"><img src="speakers/chris_barrick_110617/chris_barrick_110617_resized.jpg" alt="" /></div><div class="content">
							<h2><a href="chris_barrick_110617.html">The Vanishing Gradient Problem and Its Solutions</a></h2>
							<p>
								This talk will review the vanishing gradient problem in deep learning and some heuristics to avoid it. The topics discussed will include Xavier and He weight initialization, ReLU activation and its variations, batch normalization, and self-normalizing networks.
							</p>

							<p style="font-size: 12px;"><strong>Chris Barrick</strong>
								is a 2nd year master's student in the Institute for Artificial Intelligence at UGA, where he develops models for solar radiation forecasting. Before grad school, Chris received bachelor's degrees from UGA studying both Computer Science and Cognitive Science.
							</p>
						</div>
					</section>



					<footer id="footer">
						<ul class="icons">
								<li><a href="mailto:deeplearningatuga@gmail.com?Subject=Deep Learning @ UGA" class="icon fa-envelope-o"><span class="label">Email</span></a></li>
						</ul>
						<ul class="copyright">
							<li>&copy; EDS@UGA</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
						</ul>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>